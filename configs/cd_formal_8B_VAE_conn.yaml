run_name: vae_diffusion_experiment

trainer:
  run_name: ${run_name}
  do_train: true
  num_train_epochs: 1000
  per_device_train_batch_size: 64
  gradient_accumulation_steps: 1
  bf16: true  # This is amp, we want it now
  save_strategy: steps
  save_steps: 2500
  learning_rate: 1e-4
  max_grad_norm: 1.0
  weight_decay: 0.02
  # warmup_ratio: 0.003
  warmup_steps: 1000
  lr_scheduler_type: constant
  logging_steps: 1
  ddp_backend: nccl
  #deepspeed: ds_config_zero3.json
  #fsdp: none
  fsdp: full_shard auto_wrap
  fsdp_config:
    backward_prefetch: backward_pre
    forward_prefetch: true
    cpu_ram_efficient_loading: true
    sync_module_states: true
    transformer_layer_cls_to_wrap:
      - LlamaDecoderLayer
    use_orig_params: true
    activation_checkpointing: true
  accelerator_config:
    dispatch_batches: false
  gradient_checkpointing: false
  report_to: wandb
  output_dir: ckpt/${run_name}
  dataloader_num_workers: 16
  remove_unused_columns: false
  disable_tqdm: false
  adam_beta1: 0.9
  adam_beta2: 0.95
  overwrite_output_dir: True
  per_device_eval_batch_size: 1
  eval_strategy: "no"


model:
  llm_model_name_or_path: meta-llama/Llama-3.1-8B
  use_flash_attn: true
  freeze_tokenizer: true
  discrete: false
  use_query_embeddings: false
  num_codebooks: 8
  patch_embedding_length: 64
  block: false
  cos_loss: 1.0
  emu2_expr: false
  subsampling: false

dataset:
  train_file: data/train.jsonl
  val_file: data/val.jsonl
  local: data/
  num_canonical_nodes: 120
  shuffle: true
  per_device_batch_size: ${trainer.per_device_train_batch_size}

ae:
  lora_r: 512
  lora_alpha: 256
  lora_dropout: 0.05
  mean_compression_rate: 1
  fixed_mem_size: 3
  icae_ckpt: "checkpoints/vae_model/model.safetensors"
  scale_factor: 0.2154
  shift_factor: 0.2192

log_dir: wandb/logs
wandb_dir: wandb
allow_resume: true
